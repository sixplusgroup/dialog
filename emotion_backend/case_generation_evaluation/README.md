# 注意
python==3.8.8
pip==21.3.1
一些文件太大 不能传到github上去 故ignore掉 请自行下载 放置在本项目的根目录下  
infodata.json    https://box.nju.edu.cn/f/50ed6964e7c1413fb5cb/  
models/    https://box.nju.edu.cn/d/38fef6017a14466785a9/  

# 客服声纹库建立

- 案例生成子系统的前提条件：
  - 录音文件按客服id组织
  - 拥有每一位客服的声纹文件
  - 录音文件都是wav
- 后续做客服管理 添加客服时必须上传声纹文件
- 所以不需要冷启动分析建立客服声纹库
  - 而且不然答辩时也容易被怼，怎么确保客服声纹库的准确性
- 我用手动滚雪球的方法建立起声纹库（做的我实在是恶心，花了好长时间）
  - 取一位客服的声纹，进行声纹识别
  - 看识别比率，听录音，找到识别比率最低的是该客服的录音
  - 大于等于这个比率的录音文件都是该客服的
  - 如果把很多客户识别成了该客服，则提高判断声纹相似的阈值，重复上述过程
  - 不再处理已归为已知客服的录音
  - 在剩下的录音中再找一位客服的声纹，重复上述过程
  - 最后剩下的是全机器音的和全客户音的，随机分配给各个客服
  - 把原始录音按客服id组织

# 案例生成流程

原始录音文件放在TODO/wav目录中，生成结果在DONE目录中

运行run.sh

- 用convert_wavs.py处理原始录音使其可用，新录音在wav_new目录
- 用delete_less_than_one_mbyte.py删除小于1MB的新录音，为全机器音的录音
- 用main.py生成案例
  - 对新录音
  - 调用process函数
    - 转文本
    - 分割
    - 导出chunks
    - 情绪识别
    - 声纹识别
    - 返回5个列表speaker, character, sentences, audio_emotions, text_emotions和index
  - 用关键词识别再次过滤全机器音，忽略掉该录音
  - 确定哪个speaker是客服
    - 主要看识别比率(声纹识别出的数量/该speaker的数量)的大小
    - 如果每个speaker识别比率都为0，认为全都是客户
      - 按照系统的前提假设，知道这是哪个客服的，也拥有该客服的声纹，所以识别哪个是客服只用声纹，不用关键词识别
      - 人肉滚雪球时发现，这种情况一般是只剩下的客户说的话，而且不多，就几条，没有很大影响。而且关键词也难做，不像机器的关键词好找
  - 把该speaker对应的character改成客服，其余改成客户
  - 情绪融合 返回二维列表 第一列是音频和文本情绪融合后的情绪 第二列是情绪渲染后的情绪（同一个人连续说的话，渲染为相同情绪）
  - 把7个列表组合，输出成csv
  - 生成响度csv
  - 把结果移动到DONE目录，内容如下，均按客服id组织
    - wav 新录音文件
    - chunks 分割录音文件
    - speaker_role_text_emotion_csvs 主csv文件
    - loudness 响度csv文件

# 后续完善

- 把案例评价集成进去

- 做的更像一个应用，使上传TODO内容和运行批处理系统更容易